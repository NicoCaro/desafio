{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msngo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike Challenge: Predicción de precios de insumos básicos en Chile \n",
    "\n",
    "**Nicolás Caro**\n",
    "\n",
    "En este desafío vamos a ver si somos capaces de **predecir el precio de un insumo básico, \n",
    "como la leche, a partir de variables climatológicas y macroeconómicas**. \n",
    "\n",
    "No siempre estos datos nos entregan toda la información que nos gustaría, como por ejemplo, señales claras \n",
    "del avance de la sequía a lo largo del país. Sin embargo, nos permiten entender otro tipo de \n",
    "efectos,  como  movimientos  en  ciertos  sectores  de  la  economía.  \n",
    "\n",
    "En  esta  línea,  te  iremos guiando para construir un análisis y algunos modelos que nos ayuden a concluir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos: Precipitaciones, Indicadores Económicos Banco Central \n",
    " \n",
    "* Cargar  archivo  precipitaciones.csv  con  las  precipitaciones  medias  mensuales registradas entre enero 1979 y abril 2020. (Unidad: mm). \n",
    "\n",
    "Usamos el argumento `parse_dates` para trabajar directamente con objetos tipo `datetime`. Más aún, el argumento `index_col` permite configurar dichas fechas como *indice* o *id* de la base. Trabajar de esta manera proporciona mejores herramientas de selección y manipulación de datos. \n",
    "\n",
    "Se espera que el `DataFrame` resultante se comporte como un conjunto de series de tiempo, indexadas por `date`. Donde cada columna representa una región."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "precipitaciones = pd.read_csv('https://raw.githubusercontent.com/SpikeLab-CL/desafio_spike_precios/c01b3f9aa6d5a5359cc6d390f57ebb53f5bb7885/precipitaciones.csv',\n",
    "                              parse_dates=['date'], index_col='date')\n",
    "precipitaciones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos si en efecto se tienen datos entre enero 1979 y abril 2020. Para ello, se ordena el dataset según fecha y se observan el primer y último registro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitaciones.sort_values(by='date', inplace=True) \n",
    "precipitaciones.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cargar archivo banco_central.csv con variables económicas.\n",
    "\n",
    "Se carga esta base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banco_central = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/SpikeLab-CL/desafio_spike_precios/main/banco_central.csv')\n",
    "\n",
    "# Ordenamos los valores para asegurar un codigo reproducible\n",
    "banco_central.sort_values(by='Periodo', inplace=True)\n",
    "banco_central.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la columna `Periodo` presenta *aparentemente* un formato del tipo `%Y-%m-%d %H:%M:%S %Z` es decir *año - mes - dia hora:minuto:segundo Zona_horaria*. Al igual que el caso anterior, nos conviene trabajar con objetos tipo `datetime` para mejor control sobre los datos. \n",
    "\n",
    "Se utiliza `pd.to_datetime` entregando el formato y la columna `Periodo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pd.to_datetime(banco_central.Periodo)\n",
    "except ValueError as e:\n",
    "    print(f'Format error found: \\n {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, tenemos fechas con formato `2020-13-01 00:00:00 UTC` donde claramente `13 > 12`, la cantidad de meses de un año. Al estudiar la estructura de los datos, vemos que en efecto, tienen la forma `%Y-%m-%d %H:%M:%S %Z`, más especificamente, `%Y-%m-01 00:00:00 UTC`donde `%Y` y `%m` representan año y mes. En este contexto, se busca identificar aquellos indicies para los cuales hay meses mayores a 12. Para ello se ejecuta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indexes = []\n",
    "for i in range(banco_central.shape[0]):\n",
    "    try: \n",
    "        pd.to_datetime(banco_central.Periodo.iloc[i]) \n",
    "    except: \n",
    "        idx = banco_central.iloc[i].name \n",
    "        print(f'Error at row {i}, with index {idx}')\n",
    "        bad_indexes.append(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, solamente la observación `613` con identificador `89` presenta dicho problema. Esto se tratará posteriormente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de datos: Creación de variables\n",
    "\n",
    "* Realiza un análisis exploratorio de la base de datos, ¿Qué puedes decir de los datos, sus distribuciones, valores faltantes, otros? ¿Hay algo que te llame la atención? \n",
    "\n",
    "Comenzamos con la base `precipitaciones`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitaciones.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base tiene 496 registros, estos están indexados por fecha desde  `1979-01-01` hasta `2020-04-01`. Con lo que se confirma la primera revisión que se hizo de los datos. Hay información sobre 8 columnas, cada una de las cuales representa una región de Chile. A priori no se aprecian valores faltantes, el tipo de dato para cada región es del tipo `float64`.\n",
    "\n",
    "Se estudia la estructura preliminar de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitaciones.describe().T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La media de precipitaciones coincide con la ubicación geográfica de cada región. De la misma manera, los máximos y minimos tienen relación con el panorama que uno esperaría a priori para cada región. \n",
    "\n",
    "En cuanto a las propiedades estadísticas del conjunto de datos, se puede observar que el grado de dispersión en la caida de lluvias es alto, pues la desviación estandar es en la mayoria de los casos mayor o bastante similar a la media para cada región.\n",
    "\n",
    "Se observa la distribución de los posibles valores faltantes en el conjunto datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "msngo.matrix(precipitaciones, fontsize=15, figsize=[ \n",
    "             8, 6], ax=ax, sparkline=False);\n",
    "\n",
    "ax.set_title('Missing Values Rain Dataset', fontsize = 20); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, la visualización anterior confirma la ausencia de valores faltantes. En cuanto a los valores duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitaciones.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve que no existen entradas con valores duplicados.\n",
    "\n",
    "Hasta ahora, tanto los estadísticos de tendencia central como las visualizaciones e inspecciones iniciales, indican que el conjunto de datos asociado a las precipitaciones se encuentra bastante *limpio*. \n",
    "\n",
    "Pasamos a estudiar las distribuciones univariadas para cada región"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=[12, 12])\n",
    "\n",
    "# Se elimina el ultimo Subplot\n",
    "ax[-1, -1:][0].remove()\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Rain Dataset: Univariate Distributions ',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.02)\n",
    "\n",
    "for axis, col in zip(ax.flatten(), precipitaciones.columns):\n",
    "    sns.histplot(precipitaciones[col], ax=axis, bins=50, stat='density',\n",
    "                 fill=False)\n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "    axis.set_ylabel('', fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.22, .22)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los datos de la base `precipitaciones` corresponden a medias *mensuales* en *mm* para cada región durante `496` meses. En este contexto, la visualizanción anterior nos muestra una fotografia **estática** y **global** de lo que ocurrió durante este periodo (extenso) de tiempo y no nos permite observar la **dinamica** anual o sobre algún periodo significativo (ej: sabemos que en chile, durante el verano llueve menos que en invierno). No obstante, se puede apreciar que las regiones `Coquimbo`, `Valparaiso`,`Metropolitana_de_Santiago`, `Libertador_Gral__Bernardo_O_Higgins` y `Maule`son en general más secas que `Biobio`, `La_Araucania` y `Los_Rios`. Siendo estás últimas dos regiones las más lluviosas del conjunto de datos. \n",
    "\n",
    "Podemos visualizar la distribución de lluvias desde la perspectiva mensual. Para ello, reagrupamos los datos según mes y vemos la distribución de lluvias por región:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "months = precipitaciones.index.month_name()\n",
    "query_df = precipitaciones.groupby(by=months)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=3, figsize=[\n",
    "                       12, 12], sharex=True, sharey=True)\n",
    "\n",
    "for axis, mon in zip(ax.flatten(), months):\n",
    "    \n",
    "    axis.set_title(mon, fontsize = 15)\n",
    "    sns.boxplot(data=query_df.get_group(mon), orient='h',\n",
    "                palette='pastel', ax=axis) \n",
    "    axis.tick_params(labelsize = 13)\n",
    "    \n",
    "fig.suptitle('Rain Dataset: Distributions per Month',\n",
    "             fontsize=20,\n",
    "             x=0.4,\n",
    "             y=1.02);\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se observa la progresión y regresión estadística de las lluvias en cada región para un año *estadístico*. Claramente se aprecia como las lluvias son menores en los meses de verano para aumentar en otoño e invierno sin importar la región. \n",
    "\n",
    "Por último, observamos algunas medidas generales para cada mes en todo el terrotorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = precipitaciones.groupby(by = precipitaciones.index.month)\n",
    "\n",
    "agg_df = grouped_df.mean().T \n",
    "agg_df.columns = months.unique()\n",
    "\n",
    "monthly_metrics = agg_df.describe()\n",
    "monthly_metrics.drop('count', inplace=True)\n",
    "monthly_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde se aprecia una alta variación a lo largo del territorio nacional para cada mes, además de una mayor concentración de las lluvas durante los meses de mayo, junio, julio y agosto. \n",
    "\n",
    "De manera gráfica esto se expresa según:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[12, 10])\n",
    "\n",
    "sns.boxplot(data=agg_df, ax=ax, orient='h', palette='pastel')\n",
    "ax.set_title('Rain Distribution Over a Statistical Year', fontsize=20)\n",
    "ax.tick_params(labelsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la base de precipitaciones:\n",
    "\n",
    "1. ¿Qué puedes decir de los datos, sus distribuciones, valores faltantes, otros? \n",
    "\n",
    "Se puede inferir que independiente de la región, cada año presenta una gran dispersión en cuanto a los *mm* de lluvia caidos. Las distribuciones observadas concuerdan con el comportamiento esperado para cada región y año estadístico. Desde el punto de vista técnico, el conjunto de datos no presenta problemas estructurales del tipo *valores perdidos*, *valores duplicados* ni *incosistencias* en *tipos de datos*, ni *distribuciones*.\n",
    "\n",
    "\n",
    "2. ¿Hay algo que te llame la atención? \n",
    "\n",
    "Me llamó la atención que la gran parte de los *mm* de lluvia caidos durante el año se concentran a nivel pais solo en 4 meses (mayo - agosto), además de la alta densidad de valores cercanos a 0 *mm* desde Coquimbo hasta el Bio-Bio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos con la base `banco_central`. \n",
    "\n",
    "Anteriormente observamos que había una observación cuyo mes estaba identificado como `13`. Estudiaremos la estructura del dataset para ver si este dato erroneo es remediable. Para ello, estudiamos la cantidad de entradas por año. \n",
    "\n",
    "Primero aislamos la observación con problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_obs = banco_central.loc[89].copy()\n",
    "banco_central.drop(index=89, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego configuramos `Perdiodo` como indice de la base y transformamos a formato `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banco_central.set_index(keys='Periodo',inplace=True)\n",
    "banco_central.index = pd.to_datetime(banco_central.index)\n",
    "banco_central.index = banco_central.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente agrupamos por año y vemos la cantidad de entradas por cada año. Para esto se selecciona una columna y se extrae la cantidad de observaciones (con o sin datos) de cada año para dicha variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_df = banco_central.groupby(banco_central.index.year)\n",
    "agg_df = query_df['Precio_de_la_onza_troy_de_oro_dolaresoz'].describe()['count']\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que en general se tienen 12 entradas por año, una asocidad a cada mes. La excepción son los años `2018`, `2019` con 13 entradas y `2020` con 11. Podemos asumir que la entrada con mes `13` asilada previamente, corresponde al mes 12 del año 2020 con alta probabilidad (dada la estructura del dataset). Para confirmar esto, podemos agrupar por mes y ver si el mes 12 tiene menos entradas asociadas por año:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_counts(df=banco_central):\n",
    "    \"\"\"Returns a pd.Series object summarizing entries per month.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        df (pd.DataFrame): dataframe to work with, It's intended to have the same \n",
    "            structure as the default dataset.\n",
    "\n",
    "        Returns:\n",
    "        pd.Series: df summary.\n",
    "\n",
    "    \"\"\"\n",
    "    # We count the number of entries\n",
    "    def entry_counter(x): return x.shape[0]\n",
    "\n",
    "    query_df = df.groupby(df.index.month)\n",
    "    agg_df = query_df.aggregate(entry_counter)\n",
    "\n",
    "    agg_df.index = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                    'July', 'August', 'September', 'October', 'November',\n",
    "                    'December']\n",
    "\n",
    "    df_res = agg_df.iloc[:, 0].copy()\n",
    "    df_res.name = 'Entries'\n",
    "\n",
    "    return pd.DataFrame(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en efecto el mes 12 (December) posee 50 entradas, a diferencia de las 51 presentes para cada mes, excepto el mes 8 (August). La presencia de 53 entradas en el mes 8 se puede deber a observaciones duplicadas. \n",
    "\n",
    "Quitamos valores duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banco_central.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_month_counts(df = banco_central)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior observamos que en efecto el mes de agosto pasa a tener los 51 registros esperados. Finalmente agregamos la observación aislada como mes 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_obs.Periodo = '2020-12-01'\n",
    "iso_obs.name = pd.to_datetime(iso_obs.Periodo) \n",
    "iso_obs.drop('Periodo', inplace=True) \n",
    "\n",
    "banco_central = banco_central.append(iso_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_month_counts(df = banco_central)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que confirmamos que se ha agregado el dato de manera correcta.\n",
    "\n",
    "A continuación se estudia un poco más a fondo la estructura del dataset. Comenzamos por chequear los tipos de dato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que coincida con el indice de precipitaciones\n",
    "banco_central.index.name = 'date'\n",
    "banco_central.sort_index(inplace=True)\n",
    "banco_central.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminarmente, se puede apreciar que existen 84 variables, y 612 registros. Se posee información entre 1970-01-01 al 2020-12-01. \n",
    "\n",
    "En general las variables son reconocidas como tipo `object`, es decir, no se puede reconocer un tipo de dato especifico para dichas variables. Además se observa que la cantidad de valores no nulos por columna es relativamente bajo. \n",
    "\n",
    "Verificamos la estructura de valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msngo.matrix(df=banco_central, fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el conjunto de datos está ordenado, podemos inferir que para los datos más cercanos al 2020 son aquellos con mayor información recolectada. Se puede ver además que existe una gran cantidad de valores faltantes (en blanco se muestran los valores faltantes, en negro los valores presentes).\n",
    "\n",
    "Estudiamos más en profundidad las variables involucradas en este dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banco_central.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos anteriormente, en su mayoria estas variables son reconocidas como objetos tipo `object`. Buscamos transformar dichas columnas a un tipo de datos que se condiga con la información de la base. \n",
    "\n",
    "Una inspección simple del dataset muestra que en su mayoria, los valores en las columnas reconocidas como `object` poseen el formato `xxx.yyy.zzz`. En efecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banco_central.select_dtypes('object').dropna().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos eliminar los puntos para obtener datos del tipo numerico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = banco_central.select_dtypes('object').columns\n",
    "\n",
    "banco_central[obj_cols] = banco_central[obj_cols].apply(\n",
    "    lambda x: x.str.replace(\".\", \"\", regex=False))\n",
    "\n",
    "err_cols = []\n",
    "for col in banco_central.columns:\n",
    "    try:\n",
    "        banco_central[col] = banco_central[col].astype(float) \n",
    "    except ValueError as e: \n",
    "        print(f'Columna {col} presenta el error: \\n {e}')      \n",
    "        err_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior nos indica que restan columnas para las cuales aparece el valor `'a'`. Este se reemplaza por `None`, finalmente se terminan de convertir dichas columnas al tipo `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banco_central[err_cols] = banco_central[err_cols].replace({\"a\":None}) \n",
    "banco_central[err_cols] = banco_central[err_cols].astype(float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado, ya no existen columnas reconocidas como tipo `object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(banco_central.select_dtypes('object').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los nombres de las variables nos indican que las columnas representan grupos de indices como lo son el IMACEC, PIB e indicadores estadisticos del INE entre otros.\n",
    "\n",
    "Usando la información anterior, estudiaremos los datos faltantes, agrupando según lo sugieren las columnas involucradas. Para ello, generamos la serie `missing_series`, que almacena el porcentaje de valores faltantes para cada columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = banco_central.shape[0]\n",
    "def f_perc(x): return round((x/total)*100, 1)\n",
    "\n",
    "missing_series = f_perc(banco_central.isnull().sum())\n",
    "missing_series.name = 'missing_percentage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible consultar de manera sencilla el porcentaje de valores faltantes según agrupación de interés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca una palabra de interés en las columnas de la base del banco central\n",
    "def miss_string_finder(string): return [\n",
    "    i for i in missing_series.index.values if string in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imacec\n",
    "missing_series[miss_string_finder('Imacec')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIB\n",
    "missing_series[miss_string_finder('PIB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INE\n",
    "missing_series[miss_string_finder('INE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estos ejemplos se observa un porcentaje de perdida de información superior al 50%. Se procede a estudiar cuantas variables poseen una cantidad de información mayor a ese porcentaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_series[missing_series < 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, solo 10 columnas poseen más de la mitad de los registros, teniendose en `precio_de_la_gasolina_en_eeuu_dolaresm3`,`precio_de_la_onza_troy_de_oro_dolaresoz`,`precio_de_la_onza_troy_de_plata_dolaresoz` y `precio_del_cobre_refinado_bml_dolareslibra` información casi completa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos aquellas columnas con mas del 90% de datos faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = missing_series[missing_series > 90].index\n",
    "banco_central.drop(columns=to_drop.values, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para concluir el análisis exploratorio y limpieza de la base del banco central, estudiamos las distribuciones de los datos según grupo de indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_finder(string): return [\n",
    "    i for i in banco_central.columns.values if string in i]\n",
    "\n",
    "\n",
    "def group_plotter(string):\n",
    "    '''Creates a boxplot containing a string-data group'''\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=[12, 10])\n",
    "\n",
    "    idx = string_finder(string)\n",
    "    df = banco_central[idx]\n",
    "\n",
    "    sns.boxplot(data=df, ax=ax, orient='h', palette='pastel')\n",
    "\n",
    "    ax.set_title(string + ' Index Group', fontsize=20)\n",
    "    ax.tick_params(labelsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imacec\n",
    "group_plotter('Imacec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se aprecian distribuciones muy distintas para estos indices, a pesar de la escala de los datos, se puede decir que se encuentran bien distribuidos. La diferencia en las medias de este grupo puede aportar varianza a la hora de predecir una variable de respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIB\n",
    "group_plotter('PIB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa escalas diferentes a pesar de pertenecer al mismo grupo de indices, es posible que incluir este grupo de variables introduzca mucho ruido a un posible modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precio bienes \n",
    "group_plotter('Precio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa un cambio de escala significativo entre distintas categorias, por tal motivo, tiene más sentido estudiar según cantidad fisica asociada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precio bienes -> onzas, m3, barril\n",
    "group_plotter('oz')\n",
    "group_plotter('m3')\n",
    "group_plotter('barril')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre el oro y la plata se aprecia una gran diferencia en distribución, siendo la plata significativamente menor. Por otra parte, el metro cubico de kerosene y parafina se comportan de manera similar en cuanto a su magnitud. Estaditicamente, la gasolina posee una media mucho más baja y concentrada. Finalente los barriles de petroleo presentan diferencias en sus medias y pocos valores fuera de rango, es probable que estas variables aporten positivamente al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices INE\n",
    "group_plotter('INE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grupo bastante heterogeneo, esto se puede relacionar a la naturaleza de cada indicador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices \n",
    "group_plotter('Indice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general se observa un buen comportamiento y similitud en escala, excepto para el indicie de producción industruial de electricidad, gas y agua. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué puedes decir de los datos, sus distribuciones, valores faltantes, otros? \n",
    "\n",
    "A diferencia de la base de precipitaciones, la base del banco central presenta datos heterogeneos, esto se ve al observar los indicadores estadisticos de dispersión. En cuanto a los valores faltantes, esta base consta en su mayoria de elementos de dicho tipo. Más aún, la escala y los procedimiento ultilizados para limpiar esta base, hacen creer que pudo haber ocurrido un error al importar o recolectar los datos.\n",
    "\n",
    "2. ¿Hay algo que te llame la atención? \n",
    "\n",
    "Me llamó la atención como la escala de los valores fue transversal al transformar a tipo de dato numerico, esto debido a la naturaleza heterogenea de los datos. En otras palabras, hubiera esperado más cambios de escala. Nuevamente, esto me hace sospechar del proceso de obtención de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización \n",
    "\n",
    "* Crea una función que permita graficar series históricas de precipitaciones para un \n",
    "rango de fechas determinado. Para esto la función debe recibir como argumentos el \n",
    "nombre de una región, fecha de inicio y fecha de término (asegúrate de verificar en \n",
    "tu función que tanto el nombre de la región como las fechas ingresadas existan en el \n",
    "dataset).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rain_plot(region_name, start_date, end_date):\n",
    "    '''Creates a lineplot to visualize rain data.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        region_name (str): Region to visualize, must be one of these:\n",
    "            'Coquimbo' 'Valparaiso' \n",
    "            'Metropolitana_de_Santiago'\n",
    "            'Libertador_Gral__Bernardo_O_Higgins' \n",
    "            'Maule' \n",
    "            'Biobio' \n",
    "            'La_Araucania'\n",
    "            'Los_Rios'\n",
    "\n",
    "        start_date (str or datetime): start date, must be >= 1979-01-01.\n",
    "        end_date (str or datetime): end date, must be <= 2020-04-01.\n",
    "\n",
    "    '''\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Date Check\n",
    "\n",
    "    s = precipitaciones.index.min()\n",
    "    e = precipitaciones.index.max()\n",
    "    date_range = pd.date_range(start=s, end=e)\n",
    "\n",
    "    t_msg = f'Invalid date, It must be between {s} and {e}'\n",
    "\n",
    "    assert start_date <= end_date, 'start_date must be <= end_date'\n",
    "    assert start_date in date_range, t_msg\n",
    "    assert end_date in date_range, t_msg\n",
    "\n",
    "    # Region Check\n",
    "\n",
    "    regs = precipitaciones.columns.unique()\n",
    "    r_msg = f'region_name must be one of these: {regs.values}'\n",
    "\n",
    "    assert region_name in precipitaciones.columns, r_msg\n",
    "\n",
    "    # Data gathering\n",
    "    data = precipitaciones[region_name].loc[start_date:end_date]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize = [15,8])\n",
    "    sns.lineplot(data=data, ax=ax)\n",
    "    \n",
    "    ax.set_title(f'Rains (mm) in {region_name}', fontsize=20)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    ax.set_xlabel('date', fontsize=18)\n",
    "    ax.set_ylabel('Rain (mm)', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usa esta función para graficar las precipitaciones para la Región Libertador General \n",
    "Bernardo O'Higgins y para la Región Metropolitana entre las fechas 2000-01-01 y 2020-01-01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_plot('Libertador_Gral__Bernardo_O_Higgins', ' 2000-01-01', '2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_plot('Metropolitana_de_Santiago', ' 2000-01-01', '2020-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿ Qué observas con respecto a estacionalidades y tendencias? \n",
    "\n",
    "Se ve un comportamiento bastante similar en ambas regiones, se hace evidente la tendencia a la baja en ambas series para los ultimos 20 años, lo cual es un posible indicador de sequia.\n",
    "\n",
    "En cuanto a la estacionalidad, esta es clara para ambas regiones, pues se observa una componente periodica con picks y valles recurrentes aunque con menor intensidad a medida que pasan los años. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea  una  función  que,  para  una  región,  grafique  múltiples  series  de  tiempo mensuales de precipitaciones, donde cada serie de tiempo corresponda a un año. La función debe recibir como argumento una lista con los años que queremos graficar (2000,  2005,..)  y  el  nombre  de  la  región.  El  eje  X  debe  indicar  los  meses  (enero, febrero, etc...).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_rain_plot(region_name, year_list):\n",
    "    '''Creates a lineplot to visualize monthly rain data.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        region_name (str): Region to visualize, must be one of these:\n",
    "            'Coquimbo' 'Valparaiso'\n",
    "            'Metropolitana_de_Santiago'\n",
    "            'Libertador_Gral__Bernardo_O_Higgins'\n",
    "            'Maule'\n",
    "            'Biobio'\n",
    "            'La_Araucania'\n",
    "            'Los_Rios'\n",
    "\n",
    "        year_list (list): List of (int or string) years start date, must be \n",
    "        >= 1979-01-01 and end_date must be <= 2020-04-01.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Date Check\n",
    "    s = precipitaciones.index.min()\n",
    "    e = precipitaciones.index.max()\n",
    "    date_range = pd.date_range(start=s, end=e)\n",
    "\n",
    "    t_msg = f'Invalid year, It must be between {s} and {e}'\n",
    "\n",
    "    for y in year_list:\n",
    "        assert str(y) in date_range, t_msg\n",
    "\n",
    "    # Region Check\n",
    "    regs = precipitaciones.columns.unique()\n",
    "    r_msg = f'region_name must be one of these: {regs.values}'\n",
    "\n",
    "    assert region_name in precipitaciones.columns, r_msg\n",
    "\n",
    "    # Data Gathering\n",
    "    get_years = [y for y in precipitaciones.index if y.year in year_list]\n",
    "    data = precipitaciones.loc[get_years, region_name]\n",
    "\n",
    "    year_series = pd.Series(data.index.year, name='year', index=data.index)\n",
    "    data = pd.concat([data, year_series], axis=1)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=[15, 8])\n",
    "    sns.lineplot(x=data.index.month_name(),\n",
    "                 y=data[region_name], hue=data.year, ax=ax, palette='rocket_r')\n",
    "\n",
    "    ax.set_title(f'Monthly Rains (mm) in {region_name}', fontsize=20)\n",
    "    ax.tick_params(labelsize=13)\n",
    "    ax.set_xlabel('Month', fontsize=18)\n",
    "    ax.set_ylabel('Rain (mm)', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usa esta función para graficar las precipitaciones para la Región del Maule durante \n",
    "los años 1982, 1992, 2002, 2012 y 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_rain_plot(\"Maule\", [1982, 1992, 2002, 2012, 2019])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué puedes concluir de estos gráficos? \n",
    "\n",
    "Se observa una clara disminución en la cantidad de precipitaciones a los largo de las decadas. Aunque en el 2002 hubo un peak en un mes normalmente seco (agosto), la norma es que los meses mas lluviosos (mayo, junion) decaen gradualmente en cuanto a la cantidad de *mm*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea una función que permita visualizar dos series históricas de PIB para un rango de \n",
    "fechas determinado. Para esto la función debe recibir como input el nombre de cada serie, fecha de inicio y fecha de término. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pib_2_series_plot(series_1_name, series_2_name, start_date, end_date):\n",
    "    '''Creates a lineplot to visualize two PIB-timeseries.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        series_1 (str): First series tio visualize.\n",
    "        series_2 (str): Second series tio visualize.\n",
    "\n",
    "        start_date (str): Start date to plot.\n",
    "        end_date (str): End date to plot. \n",
    "\n",
    "        \n",
    "\n",
    "        Allowed series values are: \n",
    "\n",
    "        'PIB_Agropecuario_silvicola',\n",
    "        'PIB_Pesca',\n",
    "        'PIB_Mineria',\n",
    "        'PIB_Mineria_del_cobre',\n",
    "        'PIB_Otras_actividades_mineras',\n",
    "        'PIB_Industria_Manufacturera',\n",
    "        'PIB_Alimentos',\n",
    "        'PIB_Bebidas_y_tabaco',\n",
    "        'PIB_Textil',\n",
    "        'PIB_Maderas_y_muebles',\n",
    "        'PIB_Celulosa',\n",
    "        'PIB_Refinacion_de_petroleo',\n",
    "        'PIB_Quimica',\n",
    "        'PIB_Minerales_no_metalicos_y_metalica_basica',\n",
    "        'PIB_Productos_metalicos',\n",
    "        'PIB_Electricidad',\n",
    "        'PIB_Construccion',\n",
    "        'PIB_Comercio',\n",
    "        'PIB_Restaurantes_y_hoteles',\n",
    "        'PIB_Transporte',\n",
    "        'PIB_Comunicaciones',\n",
    "        'PIB_Servicios_financieros',\n",
    "        'PIB_Servicios_empresariales',\n",
    "        'PIB_Servicios_de_vivienda',\n",
    "        'PIB_Servicios_personales',\n",
    "        'PIB_Administracion_publica',\n",
    "        'PIB_a_costo_de_factores',\n",
    "        'PIB'\n",
    "        \n",
    "        Start and End dates must be >= 1979-01-01 and <= 2020-04-01.            \n",
    "    '''\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Date Check\n",
    "\n",
    "    s = banco_central.index.min()\n",
    "    e = banco_central.index.max()\n",
    "    date_range = pd.date_range(start=s, end=e)\n",
    "\n",
    "    t_msg = f'Invalid date, It must be between {s} and {e}'\n",
    "\n",
    "    assert start_date <= end_date, 'start_date must be <= end_date'\n",
    "    assert start_date in date_range, t_msg\n",
    "    assert end_date in date_range, t_msg\n",
    "\n",
    "    # PIB Series name check\n",
    "    allowed_names = string_finder('PIB')\n",
    "\n",
    "    def msg(n): return f'Series {n} name must be one of these {allowed_names}'\n",
    "\n",
    "    assert series_1_name in allowed_names, msg(1)\n",
    "    assert series_2_name in allowed_names, msg(2)\n",
    "    \n",
    "    # Data Gathering\n",
    "    ser_1 = banco_central[series_1_name][start_date:end_date]\n",
    "    ser_2 = banco_central[series_2_name][start_date:end_date]\n",
    "\n",
    "    data = pd.concat([ser_1,ser_2], axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=[15, 8])\n",
    "    sns.lineplot(data=data, ax=ax)\n",
    "\n",
    "    ax.set_title(f'PIB indexes', fontsize=20)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xlabel('Date', fontsize=18)\n",
    "    ax.set_ylabel('Index Value', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Grafica las series de tiempo del PIB agropecuario y silvícola y la del PIB de Servicios \n",
    "financieros desde el 2013-01-01 hasta la fecha más reciente en que haya datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obs, basta con configruar end_date = 2020-12-01, la funcion elimina los na\n",
    "pib_2_series_plot('PIB_Servicios_financieros',\n",
    "                  'PIB_Agropecuario_silvicola', '2013-01-01', '2020-12-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué puedes decir de cada serie en particular? \n",
    "\n",
    "Se puede observar cierta componente periodica en ambos indeces, en cuanto al PIB Agropecuario, se aprecia cierta estacionalidad similar a la de lo *mm* caidos de manera anual a nivel pais. es posible que exista una alta correlación entre ambas variables. Por otra parte, el PIB de servicios financieros tiende levemente al alza superando los peaks del PIB agropecuario a medida a partir del año 2019. Esto ultimo indica un comportamiento distinto al apreciado entre el 2013 y 2018. ES posible que en 2021 y 2022 esta diferencia se acentúe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Hay alguna relación entre estas dos series?\n",
    "\n",
    "Ambas disminuyen en los mismos periodos temporales, si el PIB de servicios financieros es más estable durante el año, su magnitud en similar al peak alcanzado por el PIB agropecuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento y creación de variables \n",
    "\n",
    "* ¿Cómo podríamos evaluar la correlación entre las distintas series de tiempo y cómo se  tienen  que  correlacionar  para  entrenar  un  modelo?  ¿Mucha  correlación,  no correlacionadas, da igual? \n",
    "\n",
    "Se puede medir la correlación entre series de tiempo estudiando la correlación (valga la redundancia) entre los valores de estas, para distintas frecuencias. Es decir, podemos agregar los datos, por ejemplo, según mese, semestre, año, decada, etc... dando lugar a nuevas series de tiempo. Estas series representan un conjunto de valores con diferentes componentes de información acerca de las series de tiempo originales. Podemos así, medir la auto-correlación de los valores en cada intervalos de agregación, es decir, la similitud lineal de una serie con sigo misma para distintos intervalos de tiempo. Como tambien podemos medir correlaciones cruzadas, es decir, la similitud lineal entre ventanas temporales de una serie con las ventanas temporales de otra serie. \n",
    "\n",
    "En general, la correlación es un medición estadistica de *similitud lineal*, podemos tambien utilizar otras medidas de similitud, como por ejemplo la *información mutua*, el coeficiente de correlación de Kendall, funciones kernel, entre otras. \n",
    "\n",
    "Cuando se observan correlaciones en una base de datos, se espera que las variables explicativas se correlacionen o posean cierto grado de similitud (bajo algún criterio) con la variable a predecir. Para eliminar ruido de un modelo y hacerlo más simple, se busca también que la corelación (o similitud arbitraria) entre variables explicativas sea baja, un ejemplo cotidiano es el problema de *colinealidad* entre variables explicativas cuando se usa un modelo lineal. \n",
    "\n",
    "Actualmente se buscan modelos capaces de capturar *causalidad* en el modelo de datos, lamentablemente, las correlaciones representan medidas de similitud entre variables más bien geométricas. En este conexto, podriamos tener variabls altamente correlacionadas pero sin relación lógica alguna. Por tal motivo, la existencia de correlaciones puede ser vista como una característica positiva en un modelo de datos, mas no obligatoria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para el entrenamiento del modelo, queremos predecir el precio de la leche para el productor en Chile. Para eso, descarga el archivo precio_leche.csv y haz un merge con las bases de datos de precipitaciones y datos del Banco Central. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leche = pd.read_csv('https://raw.githubusercontent.com/SpikeLab-CL/desafio_spike_precios/main/precio_leche.csv')\n",
    "leche.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leche.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msngo.matrix(leche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base no posee valores faltanes ni duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = not leche.drop_duplicates().shape == leche.shape\n",
    "print(f'Has duplicated values? {resp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Procedemos a indexar por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {mes:num for num,mes in enumerate(leche['Mes'].unique(), start=1)}\n",
    "leche['Mes'].replace(mapping, inplace=True)\n",
    "dates = leche['Anio'].astype(str) + '-' + leche['Mes'].astype(str)\n",
    "\n",
    "leche.index = pd.to_datetime(dates)\n",
    "leche.index.name = 'date'\n",
    "\n",
    "leche.drop(columns=['Anio','Mes'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leche.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos las bases correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(precipitaciones, banco_central,\n",
    "                  left_index=True, right_index=True, how='left')\n",
    "dataset = pd.merge(leche, dataset, left_index=True, right_index=True, how='left')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea las variables: \n",
    "    * A partir de la variable fecha, crea nuevas variables para el año, mes, trimestre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Año\n",
    "dataset['year'] = dataset.index.year.astype(str) \n",
    "\n",
    "#Mes\n",
    "dataset['month'] = dataset.index.month_name().astype(str) \n",
    "\n",
    "#Trimestre\n",
    "mapping = {'January': 1, 'February': 1, 'March': 1, 'April': 2, 'May': 2, 'June': 2, 'July': 3,\n",
    "           'August': 3, 'September': 3, 'October': 4, 'November': 4, 'December': 4}\n",
    "dataset['tri'] = dataset['month'].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Lags  y  estadísticas  acumuladas  (por  ejemplo:  promedio,  varianza)  de  las variables que consideres relevantes.\n",
    " \n",
    "En primer lugar, podemos agregar la columna `y` que consta del precio de la leche mensual adelantada en un periodo. De esta forma, damos estructura al dataset, de manera tal que los datos actuales nos permitan predecir el precio de la leche del mes siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['y'] = dataset['Precio_leche'].shift(1)\n",
    "dataset.drop(labels='Precio_leche', axis = 1, inplace=True)\n",
    "dataset.sort_index(inplace=True)\n",
    "\n",
    "dataset = dataset[~dataset['y'].isna()] \n",
    "y = dataset['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el comportamiento periodico anual observado en las bases de precipitaciones y banco_central, tiene sentido calcular estadisticos anuales en este dataset. en este contexto, podemos calcular las medias simples, minimos y maximos anuales en precipitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = {'year', 'month', 'tri'}\n",
    "to_aggregate = dataset.columns.difference(summary_cols)\n",
    "\n",
    "for col in to_aggregate:\n",
    "    data = pd.concat([dataset[col].rolling(window=12).mean(),\n",
    "                         dataset[col].rolling(window=12).max(),\n",
    "                         dataset[col].rolling(window=12).min()], axis=1) \n",
    "    data.columns  = [col + '_year_mean', col + '_year_max', col + '_year_min']\n",
    "    dataset = pd.merge(dataset, data, left_index=True, right_index=True, how = 'left') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente estudiamos las correlaciones en el conjunto de datos con la variable objetivo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr()[['y']].drop('y').sort_values(by=['y'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una alta correlación con los valores del precio de la leche del año pasado, ya sean en su minimo, maximo o media. De la misma manera el imacec se correlaciona de buena manera. \n",
    "\n",
    "Pasaremos a eliminar las variables con correlación inferior en valor absoluto a 0.3. Para ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cors = dataset.corr()[['y']].drop('y')\n",
    "keep_cols = cors[np.abs(cors) >= 0.3].index \n",
    "dataset = dataset[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos aquellas observaciones con na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(axis=1,how='all') \n",
    "dataset = dataset.dropna(axis=0,how='all')  \n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiamos los valores faltantes del nuevo conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msngo.matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que hay una proción de los datos con baja cantidad de valores faltantes, dado que el conjunto de datos esta ordenado, podemos ver que esto ocurre en los últimos años. En efecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_by_year = dataset.isnull().sum(axis=1)\n",
    "nulls_by_year = nulls_by_year.groupby(nulls_by_year.index.year).sum()\n",
    "nulls_by_year.nsmallest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos dice que entre los años 2015 al 2019 no hay valores falntantes. Procedemos a generar un conjunto de entrenamiento para estos años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.loc['2015-01':'2019-12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo \n",
    "\n",
    "* Entrena un modelo que permita predecir el precio de la leche el próximo mes, en función de los datos entregados. \n",
    "\n",
    "Vamos a entrar un modelo basado en `LightGBM`, este tipo de modelos generará predicciones puntuales para el precio de la leche dentro de un mes. \n",
    "\n",
    "En general [LightGBM](https://github.com/Microsoft/LightGBM) es en metodo basado en *gradient booting* basado en arboles de descición, es decir, consiste en un modelo de ensamblaje (o *ensemble method*) en el cual más modelos son agregados de manera que en cada iteración, se entrena un nuevo modelo basado en el error del ensamblaje completo. \n",
    "\n",
    "El siguiente [tutorial](https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/full) detalla más al respecto. La ventaja de LightGBM se basa en que posee una alta velocidad de entrnamiento, poco uso de memoria, posee soporte para GPU y puede trabajar con datos de gran escala. \n",
    "\n",
    "Para implementar dicho modelo utilizaremos [optuna](https://optuna.org/) el cual es un software de optimización diseñado para la busqueda de hiperparámetros de manera dinamica. \n",
    "\n",
    "Generamos un objeto `study` donde incluimos un esquema de validación curzada. Dado que los datos están agregados y se incluye un lag `y` para predecir, podemos hacer uso de la naturaleza tabular del conjunto de datos con lo que no hay problemas (realtivos a series de tiempo) en cuanto a la partición del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    '''Objective function for the model optimization.'''\n",
    "    \n",
    "    #cross Val scheme\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        dataset, y, test_size=0.25)\n",
    "    \n",
    "    #feature engeneering\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    \n",
    "    train_x = scaler.transform(train_x)\n",
    "    valid_x = scaler.transform(valid_x)\n",
    "    \n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "    # hiper params to be tunned\n",
    "    param = {\n",
    "        \"metric\": \"rmsle\",\n",
    "        'n_estimators': trial.suggest_int('n_estimators',10,100),\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001,0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth',3,10),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"seed\": 1,\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0., 10),\n",
    "        'deterministic': True\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "\n",
    "    gbm = model.fit(train_x, train_y)\n",
    "\n",
    "    preds = gbm.predict(valid_x)\n",
    "    err = mean_squared_log_error(valid_y, preds)\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de manera paralela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué datos adicionales te gustaría tener?¿Qué datos son necesarios para que este \n",
    "modelo funcione/mejore las métricas? \n",
    "\n",
    "En general seria util poseer datos relacionados a indices agropecuarios y climaticos, para mejorar las métricas de predicción es escencial que los datos sean importados de manera correcta y presenten buena correlación / causalidad con la veriable explicativa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Cómo evalúas el resultado del modelo?¿Qué métricas tiene sentido mirar? \n",
    "\n",
    "El resultado del modelo se evalua utilizando la raiz del error logaritmico medio. En este tipo de problemas tiene sentido ver métricas del tipo l2 (RMSE) o l1 (MAE) pues cuantifican (dis)concordancia entre series o suseciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el rendimiento en un nuevo conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = lgb.LGBMRegressor(**trial.params)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    dataset, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "\n",
    "\n",
    "mod.fit(scaler.transform(train_x), train_y) \n",
    "\n",
    "y_pred = mod.predict(scaler.transform(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,10))\n",
    "\n",
    "sns.lineplot(data=test_y,ax=ax, label = 'test') \n",
    "sns.lineplot(x=test_y.index, y= y_pred, ax=ax, label = 'pred')\n",
    "\n",
    "ax.set_title('y Test vs y Pred for Milk Price',fontsize = 20)\n",
    "ax.tick_params(labelsize=15)\n",
    "ax.legend(fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos las caracteristicas más importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,10))\n",
    "lgb.plot_importance(mod, max_num_features=20, ax = ax,) \n",
    "\n",
    "ax.set_title('Feature importance', fontsize = 20)\n",
    "ax.set_xlabel('Importance',fontsize = 20)\n",
    "ax.set_ylabel('Features',fontsize = 20)\n",
    "ax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de modelos puede servir para cuantificar el efecto climatico en la economia y como esto afecta la vida de las personas de manera directa. En este sentido, se pueden crear modelos similares para otros insumos de la canasta básica y mostrarlos al público y entidades publicas para generar concsciencia ambiental."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
